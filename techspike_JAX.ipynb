{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuheonChu/Natural-Language-Processing/blob/main/techspike_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V21ovT4obedt",
        "outputId": "75ddf740-f9ee-4e6f-b6c4-7ea5d11383a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "Python 3.7.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.39-cp36-none-linux_x86_64.whl\"\n",
        "!pip install --upgrade jax "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moi-TJgYb6-8",
        "outputId": "1529a14c-ac72-456d-c37b-6786af5a04ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: jaxlib-0.1.39-cp36-none-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.7/dist-packages (0.3.23)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from jax) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax) (4.1.1)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax) (0.8.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax) (1.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax) (3.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax) (5.10.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Colab to run on TPU (avoid Google Cloud TPU install command \"jax[tpu]\"\")\n",
        "# import jax.tools.colab_tpu\n",
        "# jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "import jax\n",
        "import jax.numpy as np\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "print('JAX is running on', jax.lib.xla_bridge.get_backend().platform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFj9FXEpcHqA",
        "outputId": "3b7efbbb-84bb-4f33-df2f-7acad25dedaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX is running on gpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls ./input/nlp-getting-started/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPLtBJvQcgie",
        "outputId": "22876843-1661-4829-e155-2e247e76d930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access './input/nlp-getting-started/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Data preprocess**"
      ],
      "metadata": {
        "id": "Alo3zDSOJNa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Read data\n"
      ],
      "metadata": {
        "id": "TwoqlKJSLA61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('./input/nlp-getting-started/train.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('./input/nlp-getting-started/test.csv', encoding='utf-8')\n",
        "\n",
        "# pandas method for returning the first n rows (5 by default\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "f35bfa43-c041-4faf-8899-83591d4b1818",
        "id": "FP1j8VzuIqWj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-613f8060a3f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/nlp-getting-started/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./input/nlp-getting-started/test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './input/nlp-getting-started/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   Clean up data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uhie6STyLJNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def clean_tweet(tweet: str) -> str:\n",
        "    # Remove URLs\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    tweet = url.sub(r'',tweet)\n",
        "    \n",
        "    # Remove HTML Tags\n",
        "    html = re.compile(r'<.*?>')\n",
        "    tweet = html.sub(r'', tweet)\n",
        "    \n",
        "    # Remove Emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    tweet = emoji_pattern.sub(r'', tweet)\n",
        "    \n",
        "    # Remove punctuation\n",
        "    tweet = re.sub('([.,!?()#])', r' \\1 ', tweet)\n",
        "    tweet = re.sub('\\s{2,}', ' ', tweet)\n",
        "\n",
        "    # Lowercase all text\n",
        "    return tweet.lower()\n",
        "\n",
        "df['text'] = df['text'].apply(clean_tweet)\n",
        "test_df['text'] = test_df['text'].apply(clean_tweet)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "E4IGYufhJsbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Create our vocabulary using Keras text preprocessing tools (built on TensorFlow backend)\n",
        "\n"
      ],
      "metadata": {
        "id": "7J0eQTl0LUxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 64\n",
        "\n",
        "tokenizer = Tokenizer(num_words=None, filters='')\n",
        "tokenizer.fit_on_texts(df.text.tolist() + test_df.text.tolist())\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(df.text.tolist())\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_df.text.tolist())\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "word2idx = tokenizer.word_index"
      ],
      "metadata": {
        "id": "i-_SilcBLdMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(word2idx.items())[:10]"
      ],
      "metadata": {
        "id": "0C86tb0EL0KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Layers implementation (from scratch)**\n",
        "*   Superclass `Layer` --> Keras\n",
        "*   `nn.Module` --> PyTorch\n",
        "* Superclass `JaxModule`\n",
        "* Convolutional 1D implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "RbBYYElDMJAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "from typing import Any, Callable, Container, NamedTuple\n",
        "\n",
        "Parameter = Container[np.ndarray]\n",
        "ForwardFn = Callable[[Parameter, np.ndarray, Any], np.ndarray]\n",
        "\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "class JaxModule(NamedTuple):\n",
        "    # Dict or list contining the layer parameters\n",
        "    parameters: Parameter\n",
        "    \n",
        "    # How we operate with parameters to generate an output\n",
        "    forward_fn: ForwardFn\n",
        "    \n",
        "    def update(self, new_parameters) -> 'JaxModule':\n",
        "        # As tuples are immutable, we create a new jax module keeping the\n",
        "        # forward_fn but with new parameters\n",
        "        return JaxModule(new_parameters, self.forward_fn)\n",
        "    \n",
        "    def __call__(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
        "        # Automatically injects parameters\n",
        "        return self.forward_fn(self.parameters, x, **kwargs)\n",
        "\n",
        "    # Create a linear layer\n",
        "    def linear(key: np.ndarray, \n",
        "              in_features: int, \n",
        "              out_features: int, \n",
        "              activation = lambda x: x) -> 'JaxModule':\n",
        "        x_init = jax.nn.initializers.xavier_normal()\n",
        "        u_init = jax.nn.initializers.uniform()\n",
        "\n",
        "        key, W_key, b_key = jax.random.split(key, 3)\n",
        "        W = x_init(W_key, shape=(in_features, out_features))\n",
        "        b = u_init(b_key, shape=(out_features,))\n",
        "        params = dict(W=W, bias=b)\n",
        "        \n",
        "        def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n",
        "            return np.dot(x, params['W']) + params['bias']\n",
        "        \n",
        "        return JaxModule(parameters=params, forward_fn=forward_fn)\n",
        "\n",
        "    def flatten(key: np.ndarray) -> 'JaxModule':\n",
        "        # Reshapes the data to have 2 dims [BATCH, FEATURES]\n",
        "        def forward_fn(params, x, **kwargs):\n",
        "            bs = x.shape[0]\n",
        "            return x.reshape(bs, -1)\n",
        "        return JaxModule({}, forward_fn)\n",
        "\n",
        "    # Create a Convolutional 1D layer\n",
        "    def conv_1d(key: np.ndarray, \n",
        "            in_features: int, \n",
        "            out_features: int,\n",
        "            kernel_size: int,\n",
        "            strides: int = 1,\n",
        "            padding: str = 'SAME',\n",
        "            activation = lambda x: x) -> 'JaxModule':\n",
        "    \n",
        "        # [KERNEL_WIDTH, IN_FEATURES, OUT_FEATURES]\n",
        "        kernel_shape = (kernel_size, in_features, out_features)\n",
        "        # [BATCH, WIDTH, IN_FEATURES]\n",
        "        seq_shape = (None, None, in_features)\n",
        "        \n",
        "        # Declare convolutional specs\n",
        "        dn = jax.lax.conv_dimension_numbers(\n",
        "            seq_shape, kernel_shape, ('NWC', 'WIO', 'NWC'))\n",
        "        \n",
        "        key, k_key, b_key = jax.random.split(key, 3)\n",
        "        \n",
        "        kernel = jax.nn.initializers.glorot_normal()(k_key, shape=kernel_shape)\n",
        "        b = jax.nn.initializers.uniform()(b_key, shape=(out_features,))\n",
        "        params = dict(kernel=kernel, bias=b)\n",
        "        \n",
        "        def forward_fn(params: Parameter, x: np.ndarray, **kwargs):\n",
        "            return activation(jax.lax.conv_general_dilated(\n",
        "                x, params['kernel'], \n",
        "                (strides,), padding, \n",
        "                (1,), (1,), dimension_numbers=dn) + params['bias'])\n",
        "\n",
        "        return JaxModule(params, forward_fn)\n",
        "\n",
        "\n",
        "    key, subkey, linear_key = jax.random.split(key, 3)\n",
        "\n",
        "    # Input vector of shape [BATCH, FEATURES]\n",
        "    mock_in = jax.random.uniform(subkey, shape=(10, 32))\n",
        "    linear_layer = linear(linear_key, 32, 512)\n",
        "\n",
        "    print('Input shape:', mock_in.shape, \n",
        "          'Output shape after linear:', linear_layer(mock_in).shape)\n",
        "\n",
        "##########################################\n",
        "\n",
        "    key, subkey, conv_key = jax.random.split(key, 3)\n",
        "\n",
        "    # Input vector of shape [BATCH, FEATURES]\n",
        "    mock_in = jax.random.uniform(subkey, shape=(10, 128, 32))\n",
        "    conv = conv_1d(conv_key, 32, 512, kernel_size=3, strides=2)\n",
        "\n",
        "    print('Input shape:', mock_in.shape, \n",
        "          'Output shape after 1D Convolution:', conv(mock_in).shape)\n"
      ],
      "metadata": {
        "id": "D1Ohep-OMIzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l96tLjZPU4Oi",
        "outputId": "7629f0a8-c09b-4829-e4c7-a4b27c51ca61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/JuheonChu/Natural-Language-Processing.git"
      ],
      "metadata": {
        "id": "rH4nwBMbU4Bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faec541-65db-4e83-879b-b7aaf0974be8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Natural-Language-Processing'...\n",
            "remote: Enumerating objects: 351, done.\u001b[K\n",
            "remote: Counting objects: 100% (256/256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 351 (delta 125), reused 159 (delta 76), pack-reused 95\u001b[K\n",
            "Receiving objects: 100% (351/351), 78.37 KiB | 2.12 MiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd Natural-Language-Processing\n",
        "!ls \n",
        "!cd Natural-Language-Processing\n",
        "!ls\n",
        "!pwd\n",
        "!cd Natural-Language-Processing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTmCm0JiYSHC",
        "outputId": "6a740818-ccfa-42b5-c56e-79b3cf4a9a15"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural-Language-Processing  sample_data\n",
            "Natural-Language-Processing  sample_data\n",
            "/content\n"
          ]
        }
      ]
    }
  ]
}